# Roadmap Technique Phase 1 MVP - Application Lecture
*DurÃ©e : 4 mois (16 semaines) | Budget : 120kâ‚¬ | Scope : 5 phonÃ¨mes de base*

## 1. Architecture technique MVP

### 1.1 Stack technologique finalisÃ©e

#### Frontend (Next.js 14)
```typescript
// DÃ©pendances principales
{
  "next": "14.2.0",
  "react": "18.3.0",
  "typescript": "5.4.0",
  "tailwindcss": "3.4.0",
  "@radix-ui/react-*": "latest", // Composants Shadcn/ui
  "zustand": "4.5.0",            // State management
  "framer-motion": "11.0.0",     // Animations
  "howler": "2.2.4",            // Audio management
  "react-hook-form": "7.51.0",  // Formulaires
  "zod": "3.23.0",              // Validation
  "@tanstack/react-query": "5.28.0" // Data fetching
}
```

#### Backend & Infrastructure
```yaml
# Infrastructure MVP
Database: Supabase PostgreSQL
Authentication: Supabase Auth
Storage: Supabase Storage
Hosting: Vercel
Analytics: Vercel Analytics + Custom xAPI
Domain: app-lecture.com
SSL: Automatique via Vercel
```

#### Services externes
```typescript
interface ExternalServices {
  llm: 'Hugging Face Inference API (Gemma 3-8B)';
  tts: 'Web Speech API + Fallback ElevenLabs';
  stt: 'Web Speech API avec configuration franÃ§aise';
  images: 'Stable Diffusion via Hugging Face';
  analytics: 'Learning Locker sur VPS (prÃ©paration)';
}
```

### 1.2 Architecture de dossiers

```
â”œâ”€â”€ app/                          # Next.js App Router
â”‚   â”œâ”€â”€ (auth)/                   # Routes d'authentification
â”‚   â”‚   â”œâ”€â”€ login/
â”‚   â”‚   â””â”€â”€ register/
â”‚   â”œâ”€â”€ (dashboard)/              # Routes principales
â”‚   â”‚   â”œâ”€â”€ eleve/                # Interface Ã©lÃ¨ve
â”‚   â”‚   â”‚   â”œâ”€â”€ phoneme/[id]/     # ActivitÃ©s par phonÃ¨me
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ecran/[num]/  # 7 Ã©crans d'apprentissage
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ page.tsx
â”‚   â”‚   â”‚   â””â”€â”€ progression/      # Vue d'ensemble progression
â”‚   â”‚   â”œâ”€â”€ enseignant/           # Interface enseignant (Phase 2)
â”‚   â”‚   â””â”€â”€ parent/               # Interface parent (Phase 2)
â”‚   â”œâ”€â”€ api/                      # API Routes
â”‚   â”‚   â”œâ”€â”€ phonemes/             # CRUD phonÃ¨mes
â”‚   â”‚   â”œâ”€â”€ sessions/             # Gestion sessions
â”‚   â”‚   â”œâ”€â”€ ai/                   # Endpoints IA
â”‚   â”‚   â””â”€â”€ xapi/                 # Tracking xAPI
â”‚   â””â”€â”€ globals.css
â”œâ”€â”€ components/                   # Composants rÃ©utilisables
â”‚   â”œâ”€â”€ ui/                       # Shadcn/ui components
â”‚   â”œâ”€â”€ phoneme/                  # Composants spÃ©cifiques phonÃ¨mes
â”‚   â”‚   â”œâ”€â”€ EcranIdentification.tsx
â”‚   â”‚   â”œâ”€â”€ EcranLocalisation.tsx
â”‚   â”‚   â”œâ”€â”€ EcranReconnaissance.tsx
â”‚   â”‚   â”œâ”€â”€ EcranCombinaison.tsx
â”‚   â”‚   â””â”€â”€ shared/
â”‚   â”œâ”€â”€ audio/                    # Composants audio
â”‚   â”œâ”€â”€ navigation/               # Navigation et layout
â”‚   â””â”€â”€ gamification/             # Badges, progress, etc.
â”œâ”€â”€ lib/                          # Utilitaires et configuration
â”‚   â”œâ”€â”€ database/                 # Types et schemas Supabase
â”‚   â”œâ”€â”€ ai/                       # Integration LLM
â”‚   â”œâ”€â”€ audio/                    # Gestion audio
â”‚   â”œâ”€â”€ xapi/                     # Client xAPI
â”‚   â””â”€â”€ utils/                    # Helpers gÃ©nÃ©riques
â”œâ”€â”€ hooks/                        # React hooks custom
â”œâ”€â”€ stores/                       # Zustand stores
â”œâ”€â”€ types/                        # Types TypeScript
â””â”€â”€ public/                       # Assets statiques
    â”œâ”€â”€ audio/                    # Fichiers audio phonÃ¨mes
    â”œâ”€â”€ images/                   # Images et illustrations
    â””â”€â”€ icons/                    # Iconographie
```

## 2. Sprint Planning - 8 Sprints de 2 semaines

### Sprint 0 : Setup Projet (Semaine 0, prÃ©-dÃ©veloppement)
**Objectif** : Environnement prÃªt pour le dÃ©veloppement

**Livrables :**
- [x] Repository GitHub configurÃ©
- [x] Supabase project initialisÃ©
- [x] Vercel deployment configurÃ©
- [x] CI/CD pipeline basique
- [x] Design system Figma initiÃ©

```bash
# Script d'initialisation
npx create-next-app@latest app-lecture --typescript --tailwind --app
cd app-lecture
npm install @supabase/supabase-js zustand framer-motion howler
npm install -D @types/howler
```

### Sprint 1 : Fondations (Semaines 1-2)
**Objectif** : Architecture de base + authentification

**User Stories :**
- En tant qu'Ã©lÃ¨ve, je peux crÃ©er un compte avec aide d'un adulte
- En tant qu'Ã©lÃ¨ve, je peux me connecter avec mon nom et un code simple
- En tant que dÃ©veloppeur, j'ai une base de donnÃ©es structurÃ©e pour les phonÃ¨mes

**TÃ¢ches techniques :**
```typescript
// 1. Configuration Supabase
interface DatabaseSchema {
  users: {
    id: string;
    email: string;
    nom: string;
    age: number;
    niveau_scolaire: string;
    created_at: string;
  };
  
  phonemes: {
    id: number;
    symbole: string;      // [a], [m], etc.
    graphemes: string[];  // ['a', 'A', 'Ã ']
    ordre: number;        // Ordre dans la progression
    niveau: string;       // 'maternelle' | 'cp' | 'ce1'
  };
  
  student_progress: {
    id: string;
    user_id: string;
    phoneme_id: number;
    ecran_numero: number; // 1-7
    score: number;
    temps_passe: number;
    termine: boolean;
    updated_at: string;
  };
}

// 2. Configuration authentification simplifiÃ©e
interface SimpleAuth {
  loginWithCode(nom: string, codeClasse: string): Promise<User>;
  createStudent(nom: string, age: number, codeClasse: string): Promise<User>;
  getCurrentUser(): User | null;
}

// 3. Store Zustand principal
interface AppStore {
  user: User | null;
  currentPhoneme: Phoneme | null;
  currentEcran: number;
  sessionData: SessionData;
  
  // Actions
  setUser: (user: User) => void;
  startPhonemeSession: (phonemeId: number) => void;
  updateProgress: (ecran: number, score: number) => void;
}
```

**Definition of Done :**
- [ ] Connexion/inscription fonctionnelle
- [ ] Base de donnÃ©es avec 5 phonÃ¨mes (a, i, o, m, l)
- [ ] Navigation de base entre Ã©crans
- [ ] Tests unitaires pour l'auth
- [ ] DÃ©ployment automatique sur Vercel

### Sprint 2 : Ã‰cran 1 - Identification Auditive (Semaines 3-4)
**Objectif** : Premier Ã©cran fonctionnel pour tous les phonÃ¨mes

**User Stories :**
- En tant qu'Ã©lÃ¨ve, j'entends un mot et je dois dire si j'entends le son [a]
- En tant qu'Ã©lÃ¨ve, je reÃ§ois un feedback immÃ©diat sur ma rÃ©ponse
- En tant qu'Ã©lÃ¨ve, je vois ma progression sur l'Ã©cran

**Composant principal :**
```typescript
interface EcranIdentificationProps {
  phonemeId: number;
  onComplete: (score: number, timeSpent: number) => void;
}

const EcranIdentification: FC<EcranIdentificationProps> = ({ phonemeId, onComplete }) => {
  const [currentWord, setCurrentWord] = useState<WordStimulus>();
  const [score, setScore] = useState(0);
  const [questionIndex, setQuestionIndex] = useState(0);
  const [startTime] = useState(Date.now());

  // Logique d'activitÃ©
  const handleResponse = (userResponse: boolean) => {
    const isCorrect = userResponse === currentWord.containsPhoneme;
    setScore(prev => prev + (isCorrect ? 1 : 0));
    
    // Animation feedback
    playFeedbackAnimation(isCorrect);
    
    // Question suivante ou fin
    if (questionIndex < QUESTIONS_PER_SCREEN - 1) {
      setQuestionIndex(prev => prev + 1);
      loadNextWord();
    } else {
      const timeSpent = Date.now() - startTime;
      onComplete(score / QUESTIONS_PER_SCREEN, timeSpent);
    }
  };

  return (
    <div className="flex flex-col items-center justify-center min-h-screen bg-gradient-to-b from-blue-100 to-blue-200">
      <PhonemeHeader phoneme={phoneme} screenNumber={1} />
      
      <div className="text-center mb-8">
        <h2 className="text-3xl font-bold text-gray-800 mb-4">
          J'entends le son {phoneme.symbole}
        </h2>
        <p className="text-lg text-gray-600">
          Ã‰coute bien et dis-moi si tu entends le son {phoneme.symbole}
        </p>
      </div>

      <AudioPlayer 
        src={currentWord?.audioUrl} 
        autoPlay={false}
        onEnded={() => setCanAnswer(true)}
      />

      <div className="flex gap-6 mt-8">
        <ResponseButton
          type="yes"
          onClick={() => handleResponse(true)}
          disabled={!canAnswer}
        >
          ğŸ‘ Oui, j'entends {phoneme.symbole}
        </ResponseButton>
        
        <ResponseButton
          type="no"
          onClick={() => handleResponse(false)}
          disabled={!canAnswer}
        >
          ğŸ‘ Non, je n'entends pas {phoneme.symbole}
        </ResponseButton>
      </div>

      <ProgressBar 
        current={questionIndex + 1} 
        total={QUESTIONS_PER_SCREEN} 
      />
    </div>
  );
};
```

**Contenu requis :**
- 12 mots par phonÃ¨me (6 avec phonÃ¨me, 6 sans)
- Enregistrements audio professionnels
- Validation pÃ©dagogique du vocabulaire

**Definition of Done :**
- [ ] Ã‰cran 1 fonctionnel pour les 5 phonÃ¨mes
- [ ] Audio de qualitÃ© intÃ©grÃ©
- [ ] Scoring et progression sauvegardÃ©s
- [ ] Tests d'intÃ©gration
- [ ] Validation par enseignant rÃ©fÃ©rent

### Sprint 3 : Ã‰cran 2 - Localisation (Semaines 5-6)
**Objectif** : ActivitÃ© de positionnement du phonÃ¨me dans le mot

**User Stories :**
- En tant qu'Ã©lÃ¨ve, j'entends un mot et je clique oÃ¹ j'entends le son
- En tant qu'Ã©lÃ¨ve, je vois une reprÃ©sentation visuelle du mot en syllabes

**Composant spÃ©cialisÃ© :**
```typescript
interface EcranLocalisationProps {
  phonemeId: number;
  onComplete: (score: number, timeSpent: number) => void;
}

const EcranLocalisation: FC<EcranLocalisationProps> = ({ phonemeId, onComplete }) => {
  const [currentWord, setCurrentWord] = useState<WordWithPosition>();
  const [selectedPosition, setSelectedPosition] = useState<number | null>(null);

  const positions = ['DÃ©but', 'Milieu', 'Fin'];
  
  const handlePositionSelect = (position: number) => {
    setSelectedPosition(position);
    const isCorrect = position === currentWord.phonemePosition;
    
    // Feedback visuel
    playPositionFeedback(position, isCorrect);
    
    // Continuer logique...
  };

  return (
    <div className="ecran-localisation">
      <h2>Je trouve la place du son {phoneme.symbole}</h2>
      
      <WordVisualization 
        word={currentWord.text}
        syllables={currentWord.syllables}
        highlightPosition={selectedPosition}
      />
      
      <AudioPlayer src={currentWord.audioUrl} />
      
      <div className="position-buttons">
        {positions.map((pos, index) => (
          <PositionButton
            key={pos}
            position={index}
            label={pos}
            selected={selectedPosition === index}
            onClick={() => handlePositionSelect(index)}
          />
        ))}
      </div>
    </div>
  );
};
```

### Sprint 4 : Ã‰cran 3 - Reconnaissance GraphÃ¨me (Semaines 7-8)
**Objectif** : Association phonÃ¨me-graphÃ¨me

**User Stories :**
- En tant qu'Ã©lÃ¨ve, je reconnais la lettre correspondant au son
- En tant qu'Ã©lÃ¨ve, je distingue la bonne lettre parmi des distracteurs

**Innovation technique :**
```typescript
const EcranReconnaissance: FC<EcranReconnaissanceProps> = ({ phonemeId, onComplete }) => {
  const [graphemeOptions, setGraphemeOptions] = useState<GraphemeOption[]>();
  const [selectedGrapheme, setSelectedGrapheme] = useState<string | null>(null);

  // GÃ©nÃ©ration automatique de distracteurs intelligents
  const generateDistractors = (targetGrapheme: string): string[] => {
    const similarGraphemes = GRAPHEME_SIMILARITY_MAP[targetGrapheme];
    return shuffleArray(similarGraphemes).slice(0, 3);
  };

  return (
    <div className="ecran-reconnaissance">
      <h2>Je reconnais la lettre {phoneme.symbole}</h2>
      
      <PhonemeAudio phoneme={phoneme} autoPlay />
      
      <GraphemeGrid 
        options={graphemeOptions}
        onSelect={handleGraphemeSelect}
        showFeedback={selectedGrapheme !== null}
      />
      
      <WritingPractice 
        targetGrapheme={targetGrapheme}
        onComplete={handleWritingComplete}
      />
    </div>
  );
};
```

### Sprint 5 : Ã‰cran 4 - Combinaisons CV (Semaines 9-10)
**Objectif** : Innovation majeure - fusion syllabique

**User Stories :**
- En tant qu'Ã©lÃ¨ve, je forme la syllabe "ma" en combinant "m" + "a"
- En tant qu'Ã©lÃ¨ve, je lis toutes les syllabes possibles avec ma consonne

**Composant innovant :**
```typescript
const EcranCombinaison: FC<EcranCombinaisonProps> = ({ phonemeId, onComplete }) => {
  const [consonant, setConsonant] = useState<string>();
  const [selectedVowel, setSelectedVowel] = useState<string | null>(null);
  const [formedSyllable, setFormedSyllable] = useState<string>('');
  
  const vowels = ['a', 'e', 'i', 'o', 'u', 'y'];

  const handleVowelSelect = (vowel: string) => {
    setSelectedVowel(vowel);
    const syllable = consonant + vowel;
    setFormedSyllable(syllable);
    
    // Animation de formation de syllabe
    playSyllableFormation(consonant, vowel, syllable);
  };

  const handleSyllableReading = async (syllable: string) => {
    // Test de lecture avec reconnaissance vocale simple
    const reading = await recordSyllableReading(syllable);
    const score = evaluateSyllableReading(syllable, reading);
    
    updateSyllableProgress(syllable, score);
  };

  return (
    <div className="ecran-combinaison">
      <h2>Je combine {consonant} + voyelles</h2>
      
      <SyllableBuilder 
        consonant={consonant}
        vowels={vowels}
        selectedVowel={selectedVowel}
        onVowelSelect={handleVowelSelect}
      />
      
      <SyllableDisplay 
        syllable={formedSyllable}
        animated={true}
      />
      
      <ReadingTest 
        syllable={formedSyllable}
        onRead={handleSyllableReading}
      />
      
      <SyllableGrid 
        consonant={consonant}
        completedSyllables={completedSyllables}
        onSyllableClick={handleSyllableReading}
      />
    </div>
  );
};
```

### Sprint 6 : Ã‰crans 5-7 Basiques (Semaines 11-12)
**Objectif** : ComplÃ©ter les 7 Ã©crans pour MVP

**User Stories :**
- Ã‰cran 5 : Je lis des mots simples avec le phonÃ¨me
- Ã‰cran 6 : J'Ã©cris des mots simples (input clavier)
- Ã‰cran 7 : Je lis des phrases courtes

**ImplÃ©mentation simplifiÃ©e pour MVP :**
```typescript
// Ã‰cran 5 : Lecture de mots
const EcranLecture: FC<EcranLectureProps> = ({ phonemeId, onComplete }) => {
  const [currentWord, setCurrentWord] = useState<WordToRead>();
  
  return (
    <div className="ecran-lecture">
      <h2>Je lis des mots avec {phoneme.symbole}</h2>
      
      <WordDisplay 
        word={currentWord.text}
        syllables={currentWord.syllables}
        phonemeHighlight={phoneme.symbole}
      />
      
      <AudioPlayback 
        src={currentWord.audioUrl}
        onPlay={() => trackAudioPlayback()}
      />
      
      <ReadingAssessment 
        word={currentWord}
        onComplete={handleReadingComplete}
      />
    </div>
  );
};

// Ã‰cran 6 : Ã‰criture simplifiÃ©e (clavier virtuel)
const EcranEcriture: FC<EcranEcritureProps> = ({ phonemeId, onComplete }) => {
  const [targetWord, setTargetWord] = useState<string>();
  const [userInput, setUserInput] = useState<string>('');
  
  return (
    <div className="ecran-ecriture">
      <h2>J'Ã©cris des mots avec {phoneme.symbole}</h2>
      
      <WordToWrite 
        word={targetWord}
        audioHint={true}
      />
      
      <VirtualKeyboard 
        onType={setUserInput}
        simplifiedLayout={true}
      />
      
      <WritingValidation 
        target={targetWord}
        input={userInput}
        onValidate={handleWritingValidation}
      />
    </div>
  );
};
```

### Sprint 7 : IntÃ©gration IA (Semaines 13-14)
**Objectif** : GÃ©nÃ©ration de contenu adaptatif

**User Stories :**
- En tant que systÃ¨me, je gÃ©nÃ¨re automatiquement des mots adaptÃ©s au niveau
- En tant qu'Ã©lÃ¨ve, je reÃ§ois des exercices personnalisÃ©s selon mes difficultÃ©s

**IntÃ©gration LLM :**
```typescript
// Client Hugging Face
class HuggingFaceClient {
  private apiKey = process.env.HF_API_KEY;
  private model = 'google/gemma-3-8b';

  async generatePhonemeWords(
    phoneme: string,
    difficulty: number,
    count: number,
    studentAge: number
  ): Promise<GeneratedWord[]> {
    
    const prompt = `
GÃ©nÃ¨re ${count} mots franÃ§ais simples pour un enfant de ${studentAge} ans.
Contraintes :
- Contiennent le phonÃ¨me ${phoneme}
- Niveau de difficultÃ© : ${difficulty}/5
- Vocabulaire courant adaptÃ© Ã  l'Ã¢ge
- Format JSON : [{"mot": "chat", "syllables": ["chat"], "position": "dÃ©but"}]
    `;

    const response = await fetch(`https://api-inference.huggingface.co/models/${this.model}`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        inputs: prompt,
        parameters: {
          max_new_tokens: 500,
          temperature: 0.7,
          do_sample: true
        }
      })
    });

    return this.parseAndValidateResponse(await response.json());
  }

  private async parseAndValidateResponse(response: any): Promise<GeneratedWord[]> {
    // Parsing + validation du contenu gÃ©nÃ©rÃ©
    // VÃ©rification prÃ©sence phonÃ¨me, niveau vocabulaire, etc.
  }
}

// Service d'adaptation
class AdaptationService {
  async adaptContentForStudent(
    studentId: string,
    phonemeId: number,
    screenNumber: number
  ): Promise<AdaptedContent> {
    
    // RÃ©cupÃ©ration profil Ã©tudiant
    const profile = await this.getStudentProfile(studentId);
    
    // Calcul niveau de difficultÃ© adaptÃ©
    const difficulty = this.calculateOptimalDifficulty(profile, phonemeId);
    
    // GÃ©nÃ©ration contenu via IA
    const content = await this.huggingFace.generatePhonemeWords(
      phoneme.symbole,
      difficulty,
      12, // nombre de mots
      profile.age
    );
    
    return {
      words: content,
      difficulty,
      reasoning: `AdaptÃ© selon performance prÃ©cÃ©dente: ${profile.averageScore}`
    };
  }
}
```

### Sprint 8 : Finition MVP (Semaines 15-16)
**Objectif** : Stabilisation et prÃ©paration demo

**User Stories :**
- En tant qu'Ã©lÃ¨ve, je navigue facilement entre tous les Ã©crans
- En tant qu'enseignant, je peux voir la progression de mes Ã©lÃ¨ves
- En tant que parent, je peux crÃ©er le compte de mon enfant

**TÃ¢ches finales :**
```typescript
// 1. Interface de progression Ã©lÃ¨ve
const ProgressionDashboard: FC = () => {
  const { phonemes, progress } = useStudentProgress();
  
  return (
    <div className="progression-dashboard">
      <h1>Ma progression en lecture</h1>
      
      <PhonemeGrid 
        phonemes={phonemes}
        progress={progress}
        onPhonemeClick={startPhonemeActivity}
      />
      
      <BadgeCollection 
        earnedBadges={progress.badges}
        totalPoints={progress.points}
      />
      
      <WeeklyStats 
        sessionsThisWeek={progress.weeklyStats}
      />
    </div>
  );
};

// 2. Interface basique enseignant
const TeacherDashboard: FC = () => {
  const { students } = useClassData();
  
  return (
    <div className="teacher-dashboard">
      <h1>Ma classe</h1>
      
      <StudentList 
        students={students}
        onStudentClick={viewStudentProgress}
      />
      
      <ClassStatistics 
        averageProgress={classStats.averageProgress}
        strugglingStudents={classStats.struggling}
      />
    </div>
  );
};

// 3. PWA Configuration
const PWAConfig = {
  name: 'App Lecture',
  short_name: 'Lecture',
  description: 'Apprentissage de la lecture avec mÃ©thode syllabique',
  start_url: '/',
  display: 'standalone',
  background_color: '#ffffff',
  theme_color: '#3b82f6',
  icons: [
    {
      src: '/icon-192.png',
      sizes: '192x192',
      type: 'image/png'
    },
    {
      src: '/icon-512.png',
      sizes: '512x512',
      type: 'image/png'
    }
  ]
};
```

## 3. Prototypes et validations

### 3.1 Prototypes par sprint

**Sprint 1** : Prototype navigation + auth
- Figma interactif des flux principaux
- Maquette responsive mobile/tablette
- Tests utilisateur avec 5 enfants

**Sprint 2** : Prototype Ã‰cran 1 complet
- Version fonctionnelle avec vrais contenus
- Test audio sur diffÃ©rents devices
- Validation pÃ©dagogique enseignant

**Sprint 4** : Prototype Ã‰cran 4 (innovation)
- Demo de la fusion syllabique
- Test reconnaissance vocale basique
- Mesure engagement enfants

**Sprint 8** : Prototype complet MVP
- DÃ©mo complÃ¨te 5 phonÃ¨mes
- Test avec une classe pilote
- Validation technique finale

### 3.2 MÃ©triques de validation

```typescript
interface MVPMetrics {
  technique: {
    performance: 'LCP < 2s sur tablette',
    disponibilite: '99% uptime',
    erreurs: '< 1% error rate'
  };
  
  pedagogique: {
    completion: '> 80% completion rate par Ã©cran',
    engagement: '> 15min session moyenne',
    progression: '> 70% rÃ©ussite premiÃ¨re tentative'
  };
  
  utilisateur: {
    satisfaction: '> 4/5 rating enfants',
    adoption: '> 60% utilisation hebdomadaire',
    feedback: 'Qualitative par enseignants'
  };
}
```

## 4. Configuration et outils

### 4.1 Environnement de dÃ©veloppement

```bash
# Setup initial repository
git clone https://github.com/your-org/app-lecture.git
cd app-lecture

# Installation dÃ©pendances
npm install

# Configuration environnement
cp .env.example .env.local
# Remplir les variables Supabase, Hugging Face, etc.

# Base de donnÃ©es
npm run db:setup     # CrÃ©ation tables
npm run db:seed      # DonnÃ©es de test
npm run db:migrate   # Migrations

# DÃ©veloppement
npm run dev          # Serveur de dev
npm run test         # Tests
npm run test:e2e     # Tests E2E
npm run lint         # Linting
npm run type-check   # VÃ©rification TypeScript
```

### 4.2 CI/CD Pipeline

```yaml
# .github/workflows/main.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
      - run: npm ci
      - run: npm run lint
      - run: npm run type-check
      - run: npm run test
      - run: npm run test:e2e

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      - name: Deploy to Vercel
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.ORG_ID }}
          vercel-project-id: ${{ secrets.PROJECT_ID }}
```

### 4.3 Structure des tests

```typescript
// __tests__/phoneme-activities.test.tsx
describe('ActivitÃ©s PhonÃ¨mes', () => {
  test('Ã‰cran 1 - Identification auditive', async () => {
    render(<EcranIdentification phonemeId={1} onComplete={jest.fn()} />);
    
    // Test lecture audio
    const playButton = screen.getByRole('button', { name: /Ã©couter/i });
    fireEvent.click(playButton);
    expect(mockAudioPlay).toHaveBeenCalled();
    
    // Test rÃ©ponse utilisateur
    const yesButton = screen.getByRole('button', { name: /oui/i });
    fireEvent.click(yesButton);
    
    // VÃ©rification feedback
    await waitFor(() => {
      expect(screen.getByText(/bravo/i)).toBeInTheDocument();
    });
  });

  test('Ã‰cran 4 - Combinaison syllabique', async () => {
    render(<EcranCombinaison phonemeId={4} onComplete={jest.fn()} />);
    
    // Test formation syllabe
    const vowelA = screen.getByRole('button', { name: 'a' });
    fireEvent.click(vowelA);
    
    expect(screen.getByText('ma')).toBeInTheDocument();
    expect(mockPlaySyllable).toHaveBeenCalledWith('ma');
  });
});

// __tests__/e2e/user-journey.spec.ts (Playwright)
test('Parcours complet phonÃ¨me /a/', async ({ page }) => {
  await page.goto('/eleve');
  
  // SÃ©lection phonÃ¨me
  await page.click('[data-testid="phoneme-a"]');
  
  // Ã‰cran 1
  await page.click('[data-testid="start-screen-1"]');
  await page.click('[data-testid="response-yes"]');
  await expect(page.locator('[data-testid="success-feedback"]')).toBeVisible();
  
  // Navigation Ã©cran 2
  await page.click('[data-testid="next-screen"]');
  await expect(page.locator('h2')).toContainText('Je trouve la place');
  
  // ... Test complet des 7 Ã©crans
});
```

## 5. Risques et mitigation

### 5.1 Risques techniques identifiÃ©s

| Risque | ProbabilitÃ© | Impact | Mitigation |
|--------|-------------|--------|------------|
| Performance audio mobile | Medium | High | Tests cross-platform, fallbacks MP3 |
| Reconnaissance vocale imprÃ©cise | High | Medium | Seuils tolÃ©rants, validation manuelle |
| GÃ©nÃ©ration IA inadaptÃ©e | Medium | Medium | Pipeline validation, contenu de secours |
| Latence rÃ©seau Ã©coles | High | High | PWA avec cache, mode offline |

### 5.2 Plan de contingence

```typescript
interface ContingencyPlans {
  audioIssues: {
    fallback: 'Fichiers MP3 prÃ©-enregistrÃ©s',
    detection: 'Monitoring erreurs de lecture',
    timeline: '48h pour correction'
  };
  
  aiGeneration: {
    fallback: 'Base de mots statique validÃ©e',
    detection: 'QualitÃ© score < 0.8',
    timeline: 'ImmÃ©diat (switch automatique)'
  };
  
  performanceIssues: {
    fallback: 'Version allÃ©gÃ©e sans animations',
    detection: 'LCP > 3s sur 10% devices',
    timeline: '1 semaine optimisation'
  };
}
```

## 6. Jalons et livrables

### 6.1 DÃ©mos jalons

**Semaine 4** : Demo Sprint 2
- Authentification + Ã‰cran 1 fonctionnel
- Audience : Ã‰quipe + Enseignant rÃ©fÃ©rent

**Semaine 8** : Demo Sprint 4
- 3 premiers Ã©crans complets + Ã‰cran 4 innovant
- Audience : ComitÃ© pÃ©dagogique Ã©largi

**Semaine 12** : Demo Sprint 6
- 7 Ã©crans complets pour 5 phonÃ¨mes
- Audience : Classe pilote (test rÃ©el)

**Semaine 16** : Demo MVP Final
- Application complÃ¨te + Analytics basiques
- Audience : Stakeholders + DÃ©cision Phase 2

### 6.2 CritÃ¨res de succÃ¨s Phase 1

```typescript
interface SuccessCriteria {
  fonctionnel: {
    phonemesImplemented: 5,                    // /a/, /i/, /o/, /m/, /l/
    screensPerPhoneme: 7,                      // Tous les Ã©crans
    userJourneyComplete: true,                 // Parcours bout en bout
    adaptiveContent: 'basic'                   // IA gÃ©nÃ©ration basique
  };
  
  technique: {
    performanceBudget: 'Core Web Vitals Green',
    crossPlatform: 'iOS/Android/Desktop',
    accessibility: 'WCAG 2.1 AA partial',
    offline: 'PWA basic functionality'
  };
  
  pedagogique: {
    contentValidation: 'Expert approval',
    userTesting: '20+ enfants tested',
    teacherFeedback: 'Positive overall',
    learningOutcomes: 'Measurable progress'
  };
  
  business: {
    budget: 'â‰¤ 120kâ‚¬',
    timeline: 'â‰¤ 16 semaines',
    teamSatisfaction: 'High morale',
    phase2Readiness: 'Architecture scalable'
  };
}
```

Cette roadmap dÃ©taillÃ©e vous donne un plan d'exÃ©cution concret pour dÃ©marrer immÃ©diatement le dÃ©veloppement. Chaque sprint a des livrables clairs, des critÃ¨res de rÃ©ussite mesurables et des points de validation rÃ©guliers.

**Prochaine Ã©tape recommandÃ©e** : Constitution de l'Ã©quipe et dÃ©marrage Sprint 0 dÃ¨s la validation de cette roadmap.

Souhaitez-vous que je dÃ©taille un aspect spÃ©cifique (setup technique, user stories dÃ©taillÃ©es, ou stratÃ©gie de tests) ?